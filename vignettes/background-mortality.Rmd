---
title: "Background mortality"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Background mortality}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This is the second in a series of vignettes illustrating methods for evaluating the fit and efficiency of three state oncology cost-effectiveness model structures, as described in an accompanying journal article.[1] The package is heavily dependent on [flexsurv](https://cran.r-project.org/package=flexsurv)).[2]

After fitting models, as described in `vignette("example")`, estimates of Restricted Mean Durations (RMDs) in health states can be calculated after constraining for background mortality from a given life table.

## What does constraining for background mortality mean?

### Approach 1 - Constraining only the survival function

This background mortality is assumed to limit the survival function as follows.

$$
S_{adjusted}(t) = \min \left[ S_{unadjusted}(t), S_{background}(t) \right]
$$

This is a simple approach, and has been used at least one NICE technology appraisal: "Patient longevity is always the lesser of values generated from the disease-specific survival curve (after adjustment for treatment and functional status) and the survival curve for the general population according to age and sex." [3] However, this approach does not ensure that the hazard is at least as great as the hazard of background mortality.

### Approach 2 - Constraining the hazard function

A more rigorous adjustment would be that the hazard of background mortality acts as a constraint to the unadjusted hazard.[4]

$$
h_{adjusted}(t) = \max \left[ h_{unadjusted}(t), h_{background}(t) \right]
\implies S_{adjusted}(t) = \exp \left[ - \int_0^t h_{adjusted}(u) du \right]
$$
In either case, we must assume that the original dataset was NOT subject to background mortality. Although unlikely to be true, this is a common, pragmatic assumption in cost-effectiveness models as long as background mortality is relatively insignificant during trial follow-up.

### Approach 3 - Modeling excess hazard in the original dataset

The above approaches work by adjusting extrapolations made from models fitted to data assumed to be not subject to background mortality. If - as is likely - the population in the dataset were in fact subject to background mortality, then it would be better to model that dataset using excess hazard methods rather than to seek to make adjustments to extrapolations after the fact. Further discussion of survival extrapolation incorporating general population mortality is provided by Sweeting et al.[4]

### Background mortality survival and hazard functions

Our lifetable gives us decreasing $l_x$ values at points between $t=0, ..., t_{max}$, with $l_{t_{max}}=0$. Let us define the background mortality survival function as follows.

$$
S_{gen}(t) = 1 - R \left(1 - \frac{l_{x_0 + t}}{l_{x_0}} \right)
$$

where $x_0$ is the mean age of the study population at baseline, and
$l_x$ is an appropriate lifetable.

We can also derive the average background mortality hazard for $t$ in the range $[t_1, t_2)$.

$$
h_{gen}(t) = \frac{\log[S_{gen}(t_1)] - \log[S_{gen}(t_2)]}{t_2 - t_1}
$$

## Application to three state cost-effectiveness model structures

In general, we wish to constrain the pre- and post-progression mortality (known as PPD and PPS respectively) to be at least as great as background mortality.

$$
h^{adj}_{PPD}(t) = \max \left[h_{gen}(t), h_{PPD}(t) \right] \\
h^{adj}_{PPS}(t) = \max \left[h_{gen}(t), h_{PPS}(t) \right] \\
$$

### Partitioned Survival Model (PSM)

The RMD in the PF state and time alive are integrals over the time horizon, $T$, of the survival functions of PFS and OS. Under Approach 2, the hazard functions must be constrained to be at least as great as the hazard for background mortality. The hazard for the time in PF and time alive (OS) are as follows.

$$
h^{adj}_{OS}(t) = \max \left[h_{gen}(t), h_{OS}(t) \right] \\
h^{adj}_{PFS}(t) = h_{PFS}(t) - h_{OS}(t) + h^{adj}_{OS}(t)
$$

### State Transition Models (STMs)

Formulas for the mean time in PF and OS are shown in the accompanying article in respect of hazard and survival functions for TTP, PPD and PPS endpoints, where PPS is a function of time from baseline in a Clock Forward (CF) and time from progression in a Clock Reset (CR) model.

Adjusting for background mortality requires adjusting the hazard (and equivalently survival) functions of endpoints relating to mortality, PPD and PPS. This is straightforward for PPD:

$$
h^{adj}_{PPD}(t) = \max \left[h_{gen}(t), h_{PPD}(t) \right]
$$
But it is more complex for the PPS endpoint because, in the case of the Clock Reset model, its hazard (and survival) is a function of two times rather than just one. This package does not provide integral solutions relying on continuous time for RMDs, and must instead rely on discretization.

Spreadsheet-based economic models almost always rely on discretizing time into timesteps rather than follow the integral formulas described above for continuous time. As long as timesteps are reasonably short, restricted mean duration results should remain reasonably accurate. Derided as a "kludge", half-cycle corrections are nevertheless recommended.[5]

## Illustration of calculations

### Set-up

First we load the packages we need - all of which are suggested for or imported to *psm3mkv*. With thanks again to @vbaliga for [this helpful code](https://vbaliga.github.io/posts/2019-04-28-verify-that-r-packages-are-installed-and-loaded/)).

```{r packages, message=FALSE}
# Install latest 'Nextrel' version from github, without vignettes
require("remotes")
remotes::install_github("Merck/psm3mkv",
                        ref="Nextrel",
                        build_vignettes=FALSE)

# First specify the packages of interest
packages = c("psm3mkv", "dplyr", "flexsurv", "HMDHFDplus", "remotes")

# Now load or install & load all
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)
```

### Creating the life table

In order to apply constraints to background mortality, we need some background mortality data, in the form of a lifetable. We can take this from the Human Mortality Database using the *HMDHFDplus* package mentioned above (with thanks to Robert Hettle for the recommendation). The lifetable will need to start from an assumed age at baseline. Lifetables are constructed by time in years.

```{r ltable1, eval=FALSE}
# Assumed population age at baseline (time=0)
baseage <- 51.0

# Mortality data - England & Wales, Female, 2019
mort <- HMDHFDplus::readHMDweb(CNTRY="GBRTENW",
                        item="fltper_1x1",
                        username="",
                        password=""
                        ) |>
              filter(Year==2019) |>
              select(Age, lx, dx) |>
              mutate(Timey = ceiling(Age-baseage)) |>
              filter(Timey>=0)

# The table needs to end with lx=0
mort <- dplyr::add_row(mort, Age=111, lx=0, Timey=60)
```

You will see that the above code cannot run without a login for the [Human Mortality Database](www.mortality.org). Alternatively, we could just make up a mortality table.

```{r ltable2}
mort <- tibble::tibble(
  Timey = 0:30,
  lx = 10000 * exp(-0.03 * Timey^1.1),
  dx = lx - dplyr::lead(lx),
  qx = dx/lx
  )
```

However we do it, once we have the mortality data, we can apply the SMR ($R$) and derive a lifetable from time zero as required.

```{r ltable3}
# Assumed Standardized Mortality Ratio
SMR <- 2

# Recalculate the lifetable with the SMR applied
mort$adjlx <- mort$lx
for (i in 2:length(mort$lx)) {
  mort$adjlx[i] <- mort$adjlx[i-1] * (1 - SMR * mort$qx[i-1])
}

# Ensure lx>=0
mort$adjlx[mort$adjlx<0] <- 0

# Create and view lifetable
ltable <- tibble::tibble(lttime=mort$Timey, lx=mort$adjlx)
head(ltable)
```

Next we get some patient-level data and fit the PSM and STMs. So far, we are closely following `vignette("example")`. Time is assumed to be recorded in the patient-level data in weeks, with approximately `r round(365.25/7,2)` weeks per year.

```{r datafit}
# Get some data
bosonc <- create_dummydata("flexbosms")

# We'll make the durations a lot longer
# so that they will be definitely constrained by the lifetable
bosonc <- bosonc |>
  dplyr:: mutate(
    pfs.durn = 20 * pfs.durn,
    os.durn = 20 * os.durn,
    ttp.durn = 20 * ttp.durn
  )

# Fit Royston Parmar splines models to each endpoint
allfits <- fit_ends_mods_par(bosonc)

# Pick out best distributions according to min AIC
fit.ppd <- find_bestfit_par(allfits$ppd, "aic")
fit.ttp <- find_bestfit_par(allfits$ttp, "aic")
fit.pfs <- find_bestfit_par(allfits$pfs, "aic")
fit.os <- find_bestfit_par(allfits$os, "aic")
fit.pps_cf <- find_bestfit_par(allfits$pps_cf, "aic")
fit.pps_cr <- find_bestfit_par(allfits$pps_cr, "aic")

# Bring together our preferred fits for each endpoint in a list
params <- list(ppd = fit.ppd$fit,
               ttp = fit.ttp$fit,
               pfs = fit.pfs$fit,
               os = fit.os$fit,
               pps_cf = fit.pps_cf$fit,
               pps_cr = fit.pps_cr$fit
               )
```

### Making the projections

We are skipping over any internal or external validation for the purposes of this vignette, and jump instead straight to making the survival projections. We will assume a 10 year time horizon.

First we derive a projection using integral formulae, without lifetable constraints.

```{r proj1}
# Set time horizon
thoz <- 10

# Run the calculations
proj1 <- calc_allrmds(bosonc, dpam=params, Ty=thoz)

# Present the results
res1 <- proj1$results |> dplyr::mutate(method="int", lxadj="no")
res1 |> dplyr::mutate(
  dplyr::across(c(pf, pd, os), ~ tibble::num(.x, digits = 1)))
```

In this example, there were problems estimating the RMD in the PD and OS states in the STM-CF model, which should normally be investigated.

Next we derive a projection using a discretization approximation, still without any lifetable constraints.

```{r proj2}
# Run the calculation
proj2 <- calc_allrmds(bosonc, dpam=params, Ty=thoz, rmdmethod="disc")

# Present the results
res2 <- proj2$results |>
  dplyr::mutate(method="disc", lxadj="no")
res2 |> dplyr::mutate(
  dplyr::across(c(pf, pd, os), ~ tibble::num(.x, digits = 1)))
```

Discretization has proven fairly accurate. The State Transition Model Clock Reset (STM-CR) estimate of mean time alive has reduced by just `r round(res1$os[3]-res2$os[3],1)` weeks for example, from `r round(res1$os[3],1)` to `r round(res2$os[3],1)` weeks.

Finally, we use a discretization approximation and apply the lifetable constraints.

```{r proj3}
# Run the calculations
proj3 <- calc_allrmds(bosonc, dpam=params, Ty=thoz, rmdmethod="disc", lifetable=ltable)

# Present the results
res3 <- proj3$results |>
  dplyr::mutate(method="disc", lxadj="yes")
res3 |> dplyr::mutate(
    dplyr::across(c(pf, pd, os), ~ tibble::num(.x, digits = 1)))
```

The estimate of mean time alive has reduced by `r round(res1$os[3]-res3$os[3],1)` weeks for example, from `r round(res1$os[3],1)` originally to `r round(res3$os[3],1)` weeks now.

### Comparing the results

A summary of the STM-CR estimates of mean time in PF is given in the table below.

|Model | Mean time in PF (weeks) | Change (weeks) | Mean time alive (weeks) | Change (weeks) |
|:---------------|:----------|:----------|:----------|:----------|
| No lifetable constraint, integral methods | `r round(res1$pf[3],1)` | - | `r round(res1$os[3],1)` | - |
| No lifetable constraint, discretized calculation | `r round(res2$pf[3],1)` | `r round(res2$pf[3]-res1$pf[3],1)` | `r round(res2$os[3],1)` | `r round(res2$os[3]-res1$os[3],1)` |
| With lifetable constraint, discretized calculation | `r round(res3$pf[3],1)` | `r round(res3$pf[3]-res1$pf[3],1)` | `r round(res3$os[3],1)` | `r round(res3$os[3]-res1$os[3],1)` |

In this fictional case, the application of a lifetable constraint had fairly sizeable effects on the results - although this was by design of this demonstration. The vignette focussed on the STM-CR model, although other model structures were fitted and estimated also. The package also allows application of discounting through the `discrate` optional call to `calc_allrnds()`.

## References

1. Muston D. Informing structural assumptions for three state oncology cost-effectiveness models through model efficiency and fit. In review.

2. Jackson C, Metcalfe P, Amdahl J, Warkentin MT, Sweeting M, Kunzmann K. flexsurv: Flexible Parametric Survival and Multi-State Models. Available at: https://cran.r-project.org/package=flexsurv.

3. National Institute for Health and Care Excellence. Avalglucosidase alfa for treating Pompe disease. Technology appraisal guidance [TA821]. August 24, 2022. Available from: [https://www.nice.org.uk/guidance/ta821/documents/committee-papers](https://www.nice.org.uk/guidance/ta821/documents/committee-papers)

4. Sweeting et al. Survival Extrapolation Incorporating General Population Mortality Using Excess Hazard and Cure Models: A Tutorial. Med Decis Making 2023 Aug;43(6):737-748. [DOI: 10.1177/0272989X231184247](https://doi.org/10.1177/0272989X231184247)

5. Naimark DMJ, Kabboul NN, Krahn MD. The half-cycle correction revisited: redemption of a kludge. Medical Decision Making 2013; 33(7):961-70. [DOI: 10.1177/0272989X13501558](https://doi.org/10.1177/0272989X13501558).
